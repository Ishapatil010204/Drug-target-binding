{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a1be68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost version: 2.1.4\n",
      "Build Info:\n",
      "{'BUILTIN_PREFETCH_PRESENT': False, 'CUDA_VERSION': [12, 4], 'DEBUG': False, 'MM_PREFETCH_PRESENT': True, 'THRUST_VERSION': [2, 3, 2], 'USE_CUDA': True, 'USE_DLOPEN_NCCL': False, 'USE_FEDERATED': False, 'USE_NCCL': False, 'USE_OPENMP': True, 'USE_RMM': False, 'libxgboost': 'c:\\\\Users\\\\Krishna\\\\miniconda3\\\\envs\\\\dti_gpu\\\\Lib\\\\site-packages\\\\xgboost\\\\lib\\\\xgboost.dll'}\n",
      "‚úÖ GPU (CUDA) support is available.\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "print(\"XGBoost version:\", xgb.__version__)\n",
    "\n",
    "# Check GPU support via build info\n",
    "build_info = xgb.build_info()\n",
    "print(\"Build Info:\")\n",
    "print(build_info)\n",
    "\n",
    "# --- CORRECTED LOGIC ---\n",
    "# We check the *value* of the 'USE_CUDA' key, not for a separate key named \"ON\".\n",
    "if build_info.get('USE_CUDA', False):  # .get() is a safe way to check\n",
    "    print(\"‚úÖ GPU (CUDA) support is available.\")\n",
    "else:\n",
    "    print(\"‚ùå GPU (CUDA) support not detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c1a435a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " üöÄ DRUG-TARGET INTERACTION ML TRAINING (XGBoost GPU - v4 WORKSTATION) üöÄ\n",
      " Using BindingDB_All.tsv Dataset\n",
      " Hardware: 32GB RAM, Intel Ultra 9, NVIDIA 5070 Ti\n",
      " Mode: Native Windows (XGBoost) with Skip-Logic\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "‚ôªÔ∏è SKIPPING TO STEP 4: Found pre-computed feature files!\n",
      "    Loading X_features.npy, y_class.npy, etc...\n",
      "    ‚úì Loaded features with shape: (1122930, 50)\n",
      "\n",
      "================================================================================\n",
      "STEP 4: TRAINING MACHINE LEARNING MODELS\n",
      "        üöÄ STATUS: XGBoost (Native Windows GPU) üöÄ\n",
      "================================================================================\n",
      "\n",
      "üìä Scaling features...\n",
      "    ‚úì Features scaled (mean=0, std=1)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRAINING XGBoost CLASSIFIER (Binds: Yes/No)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    Training set: 898,344 samples\n",
      "    Test set: 224,586 samples\n",
      "\n",
      "    üå≤ Training XGBoost Classifier...\n",
      "      (Using 5070 Ti... This will take a few minutes)\n",
      "[0]\tvalidation_0-logloss:0.41185\n",
      "[50]\tvalidation_0-logloss:0.27284\n",
      "[100]\tvalidation_0-logloss:0.24958\n",
      "[150]\tvalidation_0-logloss:0.24142\n",
      "[200]\tvalidation_0-logloss:0.23610\n",
      "[250]\tvalidation_0-logloss:0.23224\n",
      "[300]\tvalidation_0-logloss:0.22827\n",
      "[350]\tvalidation_0-logloss:0.22556\n",
      "[400]\tvalidation_0-logloss:0.22315\n",
      "[450]\tvalidation_0-logloss:0.22153\n",
      "[500]\tvalidation_0-logloss:0.22010\n",
      "[550]\tvalidation_0-logloss:0.21878\n",
      "[600]\tvalidation_0-logloss:0.21771\n",
      "[650]\tvalidation_0-logloss:0.21689\n",
      "[700]\tvalidation_0-logloss:0.21609\n",
      "[750]\tvalidation_0-logloss:0.21560\n",
      "[800]\tvalidation_0-logloss:0.21504\n",
      "[850]\tvalidation_0-logloss:0.21473\n",
      "[900]\tvalidation_0-logloss:0.21444\n",
      "[950]\tvalidation_0-logloss:0.21433\n",
      "[999]\tvalidation_0-logloss:0.21423\n",
      "    ‚úì Training complete!\n",
      "\n",
      "    üìà CLASSIFIER RESULTS:\n",
      "       Training Accuracy: 0.957 (95.7%)\n",
      "       Testing Accuracy: 0.912 (91.2%)\n",
      "\n",
      "    Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Binding      0.766     0.558     0.646     32354\n",
      "       Binds      0.929     0.971     0.950    192232\n",
      "\n",
      "    accuracy                          0.912    224586\n",
      "   macro avg      0.847     0.765     0.798    224586\n",
      "weighted avg      0.905     0.912     0.906    224586\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRAINING XGBoost REGRESSOR (Binding Affinity Prediction)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    Training set: 898,344 samples\n",
      "    Test set: 224,586 samples\n",
      "\n",
      "    üå≤ Training XGBoost Regressor...\n",
      "      (Using 5070 Ti... This will take a few minutes)\n",
      "[0]\tvalidation_0-rmse:1109.31161\n",
      "[50]\tvalidation_0-rmse:994.15122\n",
      "[100]\tvalidation_0-rmse:985.58686\n",
      "[146]\tvalidation_0-rmse:988.33354\n",
      "    ‚úì Training complete!\n",
      "\n",
      "    üìà REGRESSOR RESULTS:\n",
      "       Training R¬≤: 0.642\n",
      "       Testing R¬≤: 0.224\n",
      "       RMSE: 984.771 ŒºM\n",
      "       MAE: 62.287 ŒºM\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "üíæ SAVING MODELS\n",
      "--------------------------------------------------------------------------------\n",
      "    ‚úì classifier.pkl\n",
      "    ‚úì regressor.pkl\n",
      "    ‚úì scaler.pkl\n",
      "\n",
      "================================================================================\n",
      " ‚úÖ TRAINING COMPLETE! (XGBoost GPU - v4)\n",
      "================================================================================\n",
      "\n",
      "üì¶ Created files:\n",
      "    ‚úì classifier.pkl\n",
      "    ‚úì regressor.pkl\n",
      "    ‚úì scaler.pkl\n",
      "    ‚úì feature_extractor.pkl\n",
      "    ‚úì X_features.npy, y_class.npy, y_reg.npy (for re-runs)\n",
      "    ‚úì cleaned_bindingdb_data.csv\n",
      "\n",
      "üöÄ Next steps:\n",
      "    1. Your models are ready to use!\n",
      "    2. Your app can now load these .pkl files.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "OPTIMIZED BINDINGDB TRAINING SCRIPT (v4 - WORKSTATION TUNED + SKIP LOGIC)\n",
    "For file: BindingDB_All.tsv (6.23 GB)\n",
    "\n",
    "Features:\n",
    "- üöÄ GPU Accelerated (Native Windows): Uses XGBoost (tree_method='gpu_hist')\n",
    "- üß† Memory Optimized: Samples 1,500,000+ entries for 32GB+ RAM\n",
    "- üéØ Efficient TSV/CSV Loader: Fixes DtypeWarning by pre-selecting columns\n",
    "- üìä REAL PROGRESS BARS: Uses TQDM for data loading and feature extraction\n",
    "- üñ•Ô∏è Tuned for high-end CPU/GPU (Ultra 9 + 5070 Ti)\n",
    "- ‚è≠Ô∏è SKIP AHEAD: Saves/loads intermediate files (CSV, NPY) to resume training\n",
    "\n",
    "Run: python train_bindingdb_gpu_v4.py\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# === NATIVE WINDOWS IMPORTS ===\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "import xgboost as xgb # Import XGBoost\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm # ### --- CHANGED --- ### Import tqdm for progress bars\n",
    "\n",
    "# ### --- CHANGED --- ### Initialize tqdm for pandas\n",
    "tqdm.pandas() \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" üöÄ DRUG-TARGET INTERACTION ML TRAINING (XGBoost GPU - v4 WORKSTATION) üöÄ\")\n",
    "print(f\" Using BindingDB_All.tsv Dataset\")\n",
    "print(f\" Hardware: 32GB RAM, Intel Ultra 9, NVIDIA 5070 Ti\")\n",
    "print(f\" Mode: Native Windows (XGBoost) with Skip-Logic\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ==================== UTILITY FUNCTION ====================\n",
    "\n",
    "def find_column(columns, possible_names):\n",
    "    \"\"\"Find column by trying multiple possible names\"\"\"\n",
    "    for name in possible_names:\n",
    "        for col in columns:\n",
    "            if name.lower() in col.lower():\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "# ==================== STEP 1: LOAD BINDINGDB DATA ====================\n",
    "def load_bindingdb_data(filepath='BindingDB_All.tsv', sample_size=1500000):\n",
    "    \"\"\"Load and intelligently sample BindingDB TSV/CSV data\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 1: LOADING BINDINGDB DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"\\n‚ùå ERROR: File not found: {filepath}\")\n",
    "        return None\n",
    "        \n",
    "    file_size_gb = os.path.getsize(filepath) / (1024**3)\n",
    "    print(f\"\\nüìñ Reading file: {filepath}\")\n",
    "    print(f\"    File size: {file_size_gb:.2f} GB\")\n",
    "    print(f\"    Target sample size: {sample_size:,} entries\")\n",
    "    \n",
    "    print(\"\\nüßê Analyzing file header to find required columns...\")\n",
    "    \n",
    "    sep = '\\t' if filepath.endswith('.tsv') else ','\n",
    "    delimiter_name = 'TAB (\\\\t)' if sep == '\\t' else 'COMMA (,)'\n",
    "    print(f\"    Detected delimiter: {delimiter_name}\")\n",
    "    \n",
    "    try:\n",
    "        header = pd.read_csv(filepath, sep=sep, nrows=0).columns\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå CRITICAL ERROR: Could not read file header: {e}\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"    File has {len(header)} total columns.\")\n",
    "\n",
    "    smiles_col = find_column(header, ['ligand smiles', 'smiles', 'ligand_smiles'])\n",
    "    sequence_col = find_column(header, ['target sequence', 'bindingdb target chain sequence', 'sequence', 'protein sequence'])\n",
    "    ic50_col = find_column(header, ['ic50 (nm)', 'ic50', 'ic50_nm'])\n",
    "    ki_col = find_column(header, ['ki (nm)', 'ki', 'ki_nm'])\n",
    "    kd_col = find_column(header, ['kd (nm)', 'kd', 'kd_nm'])\n",
    "    \n",
    "    cols_to_load = [smiles_col, sequence_col, ic50_col, ki_col, kd_col]\n",
    "    cols_to_load = [col for col in cols_to_load if col is not None]\n",
    "    \n",
    "    if not smiles_col or not sequence_col:\n",
    "        print(\"\\n‚ùå ERROR: Could not find required SMILES or Sequence column!\")\n",
    "        return None\n",
    "        \n",
    "    if not any([ic50_col, ki_col, kd_col]):\n",
    "        print(\"\\n‚ùå ERROR: Could not find any binding affinity column (IC50, Ki, Kd)!\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\n‚úì Found required columns:\")\n",
    "    print(f\"    SMILES: {smiles_col}\")\n",
    "    print(f\"    Sequence: {sequence_col}\")\n",
    "    if ic50_col: print(f\"    IC50: {ic50_col}\")\n",
    "    if ki_col: print(f\"    Ki: {ki_col}\")\n",
    "    if kd_col: print(f\"    Kd: {kd_col}\")\n",
    "    \n",
    "    print(f\"\\nüì¶ Reading {file_size_gb:.2f}GB file in chunks...\")\n",
    "    print(f\"    This may take 5-10 minutes. Please wait...\")\n",
    "    \n",
    "    chunk_size = 1_000_000\n",
    "    chunks = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    try:\n",
    "        # ### --- CHANGED --- ### Added progress bar for file reading\n",
    "        # We need to estimate total chunks. Approx total_rows / chunk_size\n",
    "        # This is a rough estimate, but good enough for a progress bar\n",
    "        try:\n",
    "            # Quick row count (can be slow, but useful)\n",
    "            print(\"    (Performing quick row count for progress bar...)\")\n",
    "            total_rows_estimate = sum(1 for row in open(filepath, 'r', encoding='utf-8'))\n",
    "            n_chunks = int(np.ceil(total_rows_estimate / chunk_size))\n",
    "            print(f\"    (Estimated {total_rows_estimate:,} rows in {n_chunks} chunks)\")\n",
    "        except:\n",
    "            n_chunks = None # Fallback if count fails\n",
    "            print(\"    (Could not get row count, progress bar will be un-timed)\")\n",
    "\n",
    "        chunk_iter = pd.read_csv(\n",
    "            filepath, \n",
    "            sep=sep, \n",
    "            chunksize=chunk_size, \n",
    "            usecols=cols_to_load,\n",
    "            low_memory=False\n",
    "        )\n",
    "\n",
    "        for i, chunk in enumerate(tqdm(chunk_iter, total=n_chunks, desc=\"Reading 6GB File\")):\n",
    "            total_rows += len(chunk)\n",
    "            \n",
    "            if ic50_col:\n",
    "                binding_col = ic50_col\n",
    "            elif ki_col:\n",
    "                binding_col = ki_col\n",
    "            else:\n",
    "                binding_col = kd_col\n",
    "            \n",
    "            chunk = chunk.rename(columns={\n",
    "                smiles_col: 'drug_smiles',\n",
    "                sequence_col: 'protein_sequence',\n",
    "                binding_col: 'binding_value'\n",
    "            })\n",
    "            \n",
    "            chunk = chunk.dropna(subset=['drug_smiles', 'protein_sequence', 'binding_value'])\n",
    "            chunk = chunk[chunk['drug_smiles'].astype(str).str.len() > 5]\n",
    "            \n",
    "            seq_lens = chunk['protein_sequence'].astype(str).str.len()\n",
    "            chunk = chunk[(seq_lens >= 50) & (seq_lens <= 1500)]\n",
    "            \n",
    "            chunk['binding_value'] = pd.to_numeric(chunk['binding_value'], errors='coerce')\n",
    "            chunk = chunk.dropna(subset=['binding_value'])\n",
    "            \n",
    "            chunk = chunk[['drug_smiles', 'protein_sequence', 'binding_value']]\n",
    "            \n",
    "            if len(chunk) > 0:\n",
    "                chunks.append(chunk)\n",
    "\n",
    "        print(f\"\\n\\n    ‚úì Finished reading. Total rows processed: {total_rows:,}\")\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"‚ùå ERROR: No valid data was found after filtering all chunks.\")\n",
    "            return None\n",
    "            \n",
    "        print(\"    Concatenating filtered chunks...\")\n",
    "        df = pd.concat(chunks, ignore_index=True)\n",
    "        del chunks\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"    ‚úì Found {len(df):,} valid entries total.\")\n",
    "        \n",
    "        if len(df) > sample_size:\n",
    "            print(f\"\\nüìâ Sampling {sample_size:,} from {len(df):,} valid entries...\")\n",
    "            df = df.sample(sample_size, random_state=42)\n",
    "        else:\n",
    "            print(f\"\\n‚úì Using all {len(df):,} valid entries (less than target)\")\n",
    "            \n",
    "        print(f\"\\n‚úÖ DATA LOADED SUCCESSFULLY:\")\n",
    "        print(f\"    Total entries: {len(df):,}\")\n",
    "        print(f\"    Memory usage: ~{df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR loading file: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# ==================== STEP 2: CLEAN DATA ====================\n",
    "# This function is unchanged\n",
    "def clean_bindingdb_data(df):\n",
    "    \"\"\"Clean and process the already-filtered BindingDB data\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 2: CLEANING AND PROCESSING DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüßπ Processing {len(df):,} pre-filtered entries\")\n",
    "    \n",
    "    print(f\"\\nüìä Processing binding affinities...\")\n",
    "    \n",
    "    df['binding_affinity'] = df['binding_value'] / 1000.0\n",
    "    \n",
    "    initial = len(df)\n",
    "    df = df[(df['binding_affinity'] > 0.001) & (df['binding_affinity'] < 100000)]\n",
    "    print(f\"    ‚úì Valid binding values: {len(df):,} entries ({len(df)/initial*100:.1f}% kept)\")\n",
    "    \n",
    "    df['binds'] = (df['binding_affinity'] < 10).astype(int)\n",
    "    \n",
    "    print(f\"\\nüß¨ Validating protein sequences...\")\n",
    "    valid_aa = set('ACDEFGHIKLMNPQRSTVWYXU-')\n",
    "    \n",
    "    def is_valid_sequence(seq):\n",
    "        if not isinstance(seq, str) or len(seq) == 0:\n",
    "            return False\n",
    "        seq = seq.strip().upper()\n",
    "        valid_count = sum(1 for aa in seq if aa in valid_aa)\n",
    "        return valid_count / len(seq) >= 0.95\n",
    "    \n",
    "    # ### --- CHANGED --- ### Added progress_apply for sequence validation\n",
    "    print(\"    (This may take a minute...)\")\n",
    "    valid_mask = df['protein_sequence'].progress_apply(is_valid_sequence)\n",
    "    df = df[valid_mask]\n",
    "    print(f\"    ‚úì Valid sequences: {len(df):,} entries\")\n",
    "    \n",
    "    df['protein_sequence'] = df['protein_sequence'].str.strip().str.upper()\n",
    "    df['drug_smiles'] = df['drug_smiles'].str.strip()\n",
    "    \n",
    "    initial = len(df)\n",
    "    df = df.drop_duplicates(subset=['protein_sequence', 'drug_smiles'])\n",
    "    if len(df) < initial:\n",
    "        print(f\"    ‚úì Removed {initial - len(df):,} duplicates\")\n",
    "    \n",
    "    df = df[['protein_sequence', 'drug_smiles', 'binding_affinity', 'binds']].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ FINAL CLEANED DATASET:\")\n",
    "    print(f\"    Total samples: {len(df):,}\")\n",
    "    print(f\"    Binders (IC50 < 10 ŒºM): {df['binds'].sum():,} ({df['binds'].sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"    Non-binders: {(1-df['binds']).sum():,} ({(1-df['binds']).sum()/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    output_file = 'cleaned_bindingdb_data.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nüíæ Saved cleaned data to: {output_file}\")\n",
    "    \n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "# ==================== STEP 3: FEATURE EXTRACTION ====================\n",
    "# This class is unchanged\n",
    "class FeatureExtractor:\n",
    "    \"\"\"Extract numerical features from proteins and drugs\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.aa_weights = {\n",
    "            'A': 89, 'C': 121, 'D': 133, 'E': 147, 'F': 165,\n",
    "            'G': 75, 'H': 155, 'I': 131, 'K': 146, 'L': 131,\n",
    "            'M': 149, 'N': 132, 'P': 115, 'Q': 146, 'R': 174,\n",
    "            'S': 105, 'T': 119, 'V': 117, 'W': 204, 'Y': 181\n",
    "        }\n",
    "        self.amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "\n",
    "    def protein_to_features(self, sequence):\n",
    "        \"\"\"Convert protein sequence to 33 numerical features\"\"\"\n",
    "        sequence = ''.join([aa for aa in sequence if aa in self.amino_acids])\n",
    "        \n",
    "        if len(sequence) == 0:\n",
    "            return None\n",
    "            \n",
    "        total = len(sequence)\n",
    "        features = []\n",
    "        \n",
    "        aa_counts = Counter(sequence)\n",
    "        for aa in self.amino_acids:\n",
    "            features.append(aa_counts[aa] / total)\n",
    "            \n",
    "        features.append(total) # Length\n",
    "        features.append(sum([self.aa_weights.get(aa, 110) for aa in sequence])) # MW\n",
    "        features.append((aa_counts['F'] + aa_counts['W'] + aa_counts['Y']) / total) # Aromaticity\n",
    "        \n",
    "        if len(sequence) > 1:\n",
    "            dipeptides = [sequence[i:i+2] for i in range(len(sequence)-1)]\n",
    "            dipeptide_counts = Counter(dipeptides)\n",
    "            most_common = dipeptide_counts.most_common(10)\n",
    "            for i in range(10):\n",
    "                if i < len(most_common):\n",
    "                    features.append(most_common[i][1] / (total - 1))\n",
    "                else:\n",
    "                    features.append(0)\n",
    "        else:\n",
    "            features.extend([0] * 10)\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def smiles_to_features(self, smiles):\n",
    "        \"\"\"Convert SMILES to 17 numerical features\"\"\"\n",
    "        features = []\n",
    "        features.append(len(smiles)) # Length\n",
    "        features.append(smiles.count('C')) # Carbons\n",
    "        features.append(smiles.count('O')) # Oxygens\n",
    "        features.append(smiles.count('N')) # Nitrogens\n",
    "        features.append(smiles.count('S')) # Sulfurs\n",
    "        features.append(smiles.count('P')) # Phosphorus\n",
    "        features.append(smiles.count('=')) # Double bonds\n",
    "        features.append(smiles.count('#')) # Triple bonds\n",
    "        features.append(smiles.count('(')) # Branches\n",
    "        features.append(smiles.count('[')) # Atoms in brackets\n",
    "        features.append(smiles.count('@')) # Chirality\n",
    "        \n",
    "        for i in range(1, 7):\n",
    "            features.append(smiles.count(str(i)))\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def combine_features(self, protein_features, drug_features):\n",
    "        \"\"\"Combine protein and drug features\"\"\"\n",
    "        return protein_features + drug_features\n",
    "\n",
    "def prepare_ml_features(df):\n",
    "    \"\"\"Convert dataframe to ML-ready feature matrix\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 3: EXTRACTING ML FEATURES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    extractor = FeatureExtractor()\n",
    "    \n",
    "    total = len(df)\n",
    "    print(f\"\\nüî¨ Processing {total:,} samples...\")\n",
    "    \n",
    "    # ### --- CHANGED --- ### Replaced .apply() with .progress_apply()\n",
    "    print(\"    Extracting protein features (with progress bar)...\")\n",
    "    prot_features = df['protein_sequence'].progress_apply(extractor.protein_to_features)\n",
    "    \n",
    "    print(\"    Extracting drug features (with progress bar)...\")\n",
    "    drug_features = df['drug_smiles'].progress_apply(extractor.smiles_to_features)\n",
    "    \n",
    "    print(\"    Combining features...\")\n",
    "    df_features = pd.DataFrame({\n",
    "        'prot': prot_features,\n",
    "        'drug': drug_features,\n",
    "        'y_class': df['binds'],\n",
    "        'y_reg': df['binding_affinity']\n",
    "    })\n",
    "    \n",
    "    df_features = df_features.dropna().reset_index(drop=True)\n",
    "    \n",
    "    failed = total - len(df_features)\n",
    "    \n",
    "    # ### --- CHANGED --- ### Added progress bar for final combination\n",
    "    X_list = df_features.progress_apply(lambda row: extractor.combine_features(row['prot'], row['drug']), axis=1)\n",
    "    X = np.array(X_list.tolist())\n",
    "    y_class = df_features['y_class'].values\n",
    "    y_reg = df_features['y_reg'].values\n",
    "    \n",
    "    print(f\"\\n    ‚úì Successfully processed: {len(X):,} samples\")\n",
    "    if failed > 0:\n",
    "        print(f\"    ‚ö† Failed to process: {failed} samples ({failed/total*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ FEATURE EXTRACTION COMPLETE:\")\n",
    "    print(f\"    Feature matrix shape: {X.shape}\")\n",
    "    print(f\"    Memory: ~{X.nbytes / 1024**2:.1f} MB\")\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    # ### --- CHANGED --- ###\n",
    "    # Save the outputs of this step for faster re-runs\n",
    "    print(\"\\nüíæ Saving feature-extracted data for faster re-runs...\")\n",
    "    try:\n",
    "        np.save('X_features.npy', X)\n",
    "        print(\"    ‚úì Saved X_features.npy\")\n",
    "        np.save('y_class.npy', y_class)\n",
    "        print(\"    ‚úì Saved y_class.npy\")\n",
    "        np.save('y_reg.npy', y_reg)\n",
    "        print(\"    ‚úì Saved y_reg.npy\")\n",
    "        \n",
    "        with open('feature_extractor.pkl', 'wb') as f:\n",
    "            pickle.dump(extractor, f)\n",
    "        print(\"    ‚úì Saved feature_extractor.pkl\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ö† Warning: Could not save feature files: {e}\")\n",
    "    # ### --- END CHANGED --- ###\n",
    "    \n",
    "    return X, y_class, y_reg, extractor\n",
    "\n",
    "# ==================== STEP 4: TRAIN ML MODELS (XGBOOST GPU) ====================\n",
    "\n",
    "def train_models(X, y_class, y_reg, extractor):\n",
    "    \"\"\"Train XGBoost models on the GPU\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 4: TRAINING MACHINE LEARNING MODELS\")\n",
    "    print(\"        üöÄ STATUS: XGBoost (Native Windows GPU) üöÄ\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nüìä Scaling features...\")\n",
    "    scaler = StandardScaler() \n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    print(\"    ‚úì Features scaled (mean=0, std=1)\")\n",
    "    \n",
    "    # ===== TRAIN CLASSIFIER =====\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TRAINING XGBoost CLASSIFIER (Binds: Yes/No)\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n    Training set: {len(X_train):,} samples\")\n",
    "    print(f\"    Test set: {len(X_test):,} samples\")\n",
    "    \n",
    "    \n",
    "    # ### --- CHANGED --- ### Tuned parameters for higher GPU utilization\n",
    "    classifier = xgb.XGBClassifier(\n",
    "        tree_method='gpu_hist',\n",
    "        n_estimators=1000,          # From 200 -> 1000 (5x more work)\n",
    "        max_depth=12,               # From 10 -> 12 (deeper)\n",
    "        learning_rate=0.05,         # From 0.1 -> 0.05 (more careful steps)\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        early_stopping_rounds=50    # From 10 -> 50 (more room to improve)\n",
    "    )\n",
    "    \n",
    "    print(\"\\n    üå≤ Training XGBoost Classifier...\")\n",
    "    print(\"      (Using 5070 Ti... This will take a few minutes)\")\n",
    "    \n",
    "    # ### --- CHANGED --- ### Set verbose=50 to show progress\n",
    "    classifier.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        verbose=50 # Print progress every 50 trees\n",
    "    )\n",
    "    print(\"    ‚úì Training complete!\")\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, classifier.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, classifier.predict(X_test))\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print(f\"\\n    üìà CLASSIFIER RESULTS:\")\n",
    "    print(f\"       Training Accuracy: {train_acc:.3f} ({train_acc*100:.1f}%)\")\n",
    "    print(f\"       Testing Accuracy: {test_acc:.3f} ({test_acc*100:.1f}%)\")\n",
    "    print(f\"\\n    Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred,\n",
    "                                target_names=['No Binding', 'Binds'],\n",
    "                                digits=3))\n",
    "    \n",
    "    # ===== TRAIN REGRESSOR =====\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TRAINING XGBoost REGRESSOR (Binding Affinity Prediction)\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_reg, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n    Training set: {len(X_train):,} samples\")\n",
    "    print(f\"    Test set: {len(X_test):,} samples\")\n",
    "    \n",
    "    # ### --- CHANGED --- ### Tuned parameters for higher GPU utilization\n",
    "    regressor = xgb.XGBRegressor(\n",
    "        tree_method='gpu_hist',\n",
    "        n_estimators=1000,          # From 200 -> 1000\n",
    "        max_depth=12,               # From 10 -> 12\n",
    "        learning_rate=0.05,         # From 0.1 -> 0.05\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        early_stopping_rounds=50    # From 10 -> 50\n",
    "    )\n",
    "    \n",
    "    print(\"\\n    üå≤ Training XGBoost Regressor...\")\n",
    "    print(\"      (Using 5070 Ti... This will take a few minutes)\")\n",
    "    \n",
    "    # ### --- CHANGED --- ### Set verbose=50 to show progress\n",
    "    regressor.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        verbose=50 # Print progress every 50 trees\n",
    "    )\n",
    "    print(\"    ‚úì Training complete!\")\n",
    "    \n",
    "    train_r2 = r2_score(y_train, regressor.predict(X_train))\n",
    "    test_r2 = r2_score(y_test, regressor.predict(X_test))\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = np.mean(np.abs(y_test - y_pred))\n",
    "    \n",
    "    print(f\"\\n    üìà REGRESSOR RESULTS:\")\n",
    "    print(f\"       Training R¬≤: {train_r2:.3f}\")\n",
    "    print(f\"       Testing R¬≤: {test_r2:.3f}\")\n",
    "    print(f\"       RMSE: {rmse:.3f} ŒºM\")\n",
    "    print(f\"       MAE: {mae:.3f} ŒºM\")\n",
    "    \n",
    "    # Save models\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"üíæ SAVING MODELS\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    with open('classifier.pkl', 'wb') as f:\n",
    "        pickle.dump(classifier, f)\n",
    "    print(\"    ‚úì classifier.pkl\")\n",
    "    \n",
    "    with open('regressor.pkl', 'wb') as f:\n",
    "        pickle.dump(regressor, f)\n",
    "    print(\"    ‚úì regressor.pkl\")\n",
    "    \n",
    "    with open('scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(\"    ‚úì scaler.pkl\")\n",
    "    \n",
    "    # ### --- CHANGED --- ###\n",
    "    # Removed saving 'feature_extractor.pkl', as it's now saved in Step 3\n",
    "    #\n",
    "    # with open('feature_extractor.pkl', 'wb') as f:\n",
    "    #     pickle.dump(extractor, f)\n",
    "    # print(\" \t‚úì feature_extractor.pkl\")\n",
    "    # ### --- END CHANGED --- ###\n",
    "        \n",
    "    return classifier, regressor, scaler\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    bindingdb_file = 'BindingDB_All.tsv'\n",
    "    \n",
    "    # ### --- CHANGED --- ###\n",
    "    # Define all our intermediate and final file names\n",
    "    cleaned_file = 'cleaned_bindingdb_data.csv'\n",
    "    features_X_file = 'X_features.npy'\n",
    "    features_y_class_file = 'y_class.npy'\n",
    "    features_y_reg_file = 'y_reg.npy'\n",
    "    extractor_file = 'feature_extractor.pkl'\n",
    "    \n",
    "    SAMPLE_SIZE = 1_500_000 \n",
    "    \n",
    "    X, y_class, y_reg, extractor = None, None, None, None\n",
    "    df = None\n",
    "\n",
    "    # === CHECK 1: SKIP TO STEP 4 (Training) ===\n",
    "    # Check if the final feature files from Step 3 already exist\n",
    "    if all(os.path.exists(f) for f in [features_X_file, features_y_class_file, features_y_reg_file, extractor_file]):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚ôªÔ∏è SKIPPING TO STEP 4: Found pre-computed feature files!\")\n",
    "        print(f\"    Loading {features_X_file}, {features_y_class_file}, etc...\")\n",
    "        try:\n",
    "            X = np.load(features_X_file)\n",
    "            y_class = np.load(features_y_class_file)\n",
    "            y_reg = np.load(features_y_reg_file)\n",
    "            with open(extractor_file, 'rb') as f:\n",
    "                extractor = pickle.load(f)\n",
    "            print(f\"    ‚úì Loaded features with shape: {X.shape}\")\n",
    "            \n",
    "            # We also need the extractor for the train_models function\n",
    "            if extractor is None:\n",
    "                print(\"    ...Missing feature_extractor.pkl. Re-computing...\")\n",
    "                X = None # Force re-computation\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Error loading feature files: {e}. Re-computing...\")\n",
    "            X = None # Force re-computation\n",
    "\n",
    "    # === CHECK 2: SKIP TO STEP 3 (Feature Extraction) ===\n",
    "    # If we couldn't load features, check for the cleaned CSV from Step 2\n",
    "    if X is None and os.path.exists(cleaned_file):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"‚ôªÔ∏è SKIPPING TO STEP 3: Found {cleaned_file}!\")\n",
    "        print(f\"    Loading {cleaned_file}...\")\n",
    "        try:\n",
    "            df = pd.read_csv(cleaned_file)\n",
    "            if len(df) < 1000:\n",
    "                print(\"    ...File is too small. Re-running from start.\")\n",
    "                df = None # Force re-run\n",
    "            else:\n",
    "                print(f\"    ‚úì Loaded {len(df):,} cleaned samples.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Error loading {cleaned_file}: {e}. Re-running from start...\")\n",
    "            df = None # Force re-run\n",
    "\n",
    "    # === CHECK 3: Run from STEP 1 (Full Load) ===\n",
    "    # If both previous checks failed, run the full pipeline\n",
    "    if df is None and X is None:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚ñ∂Ô∏è STARTING FROM STEP 1: No valid cached data found.\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nüîç Looking for: {bindingdb_file}\")\n",
    "        \n",
    "        if not os.path.exists(bindingdb_file):\n",
    "            print(f\"\\n‚ùå File not found! Please ensure {bindingdb_file} is in this folder.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"    ‚úì Found: {bindingdb_file}\")\n",
    "        \n",
    "        df = load_bindingdb_data(bindingdb_file, sample_size=SAMPLE_SIZE)\n",
    "        if df is None or len(df) == 0:\n",
    "            print(\"\\n‚ùå Failed to load valid data. Exiting...\")\n",
    "            return\n",
    "            \n",
    "        df = clean_bindingdb_data(df)\n",
    "        if df is None or len(df) < 1000:\n",
    "            print(f\"\\n‚ùå Not enough valid data ({len(df) if df is not None else 0} samples).\")\n",
    "            return\n",
    "    \n",
    "    # --- RUN STEP 3 (if needed) ---\n",
    "    if X is None:\n",
    "        # This will run if EITHER Step 1+2 ran, OR Step 2 was skipped\n",
    "        X, y_class, y_reg, extractor = prepare_ml_features(df)\n",
    "    \n",
    "    # --- RUN STEP 4 (Training) ---\n",
    "    # This will always run, using loaded features or newly computed ones\n",
    "    if X is None:\n",
    "        print(\"\\n‚ùå Critical error: Feature matrix (X) is still None. Exiting.\")\n",
    "        return\n",
    "        \n",
    "    classifier, regressor, scaler = train_models(X, y_class, y_reg, extractor)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" ‚úÖ TRAINING COMPLETE! (XGBoost GPU - v4)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüì¶ Created files:\")\n",
    "    print(\"    ‚úì classifier.pkl\")\n",
    "    print(\"    ‚úì regressor.pkl\")\n",
    "    print(\"    ‚úì scaler.pkl\")\n",
    "    print(\"    ‚úì feature_extractor.pkl\")\n",
    "    print(\"    ‚úì X_features.npy, y_class.npy, y_reg.npy (for re-runs)\")\n",
    "    print(\"    ‚úì cleaned_bindingdb_data.csv\")\n",
    "    \n",
    "    print(\"\\nüöÄ Next steps:\")\n",
    "    print(\"    1. Your models are ready to use!\")\n",
    "    print(\"    2. Your app can now load these .pkl files.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    # ### --- END CHANGED --- ###\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚ö† Training interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n‚ùå UNEXPECTED ERROR:\")\n",
    "        print(f\"    {e}\")\n",
    "        print(\"\\nFull traceback:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dti_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
