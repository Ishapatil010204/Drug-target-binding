{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "283d81a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " 🚀 DRUG-TARGET INTERACTION ML TRAINING (XGBoost GPU - v5 IPYNB) 🚀\n",
      " Mode: Native Windows (XGBoost) with Skip-Logic\n",
      " Reading from: ../data\\raw\n",
      " Saving to: ../data\\processed and ../models\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "OPTIMIZED BINDINGDB TRAINING NOTEBOOK (v5 - IPYNB / MODULAR PATHS)\n",
    "Project: druglikeliness12\n",
    "File: notebooks/train.ipynb\n",
    "\n",
    "Features:\n",
    "- 🚀 GPU Accelerated (Native Windows): Uses XGBoost (tree_method='gpu_hist')\n",
    "- 🧠 Memory Optimized: Samples 1,500,000+ entries for 32GB+ RAM\n",
    "- 🎯 Efficient TSV/CSV Loader: Fixes DtypeWarning by pre-selecting columns\n",
    "- 📊 REAL PROGRESS BARS: Uses TQDM for data loading and feature extraction\n",
    "- 🖥️ Tuned for high-end CPU/GPU (Ultra 9 + 5070 Ti)\n",
    "- ⏭️ SKIP AHEAD: Saves/loads intermediate files (CSV, NPY) to resume training\n",
    "- 📂 MODULAR PATHS: Reads/saves to project root structure (data/, models/)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === ML IMPORTS ===\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Initialize tqdm for pandas\n",
    "tqdm.pandas() \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==================== PATH DEFINITIONS ====================\n",
    "# This notebook is in /notebooks, so root is one level up ('../')\n",
    "ROOT_DIR = '../'\n",
    "\n",
    "# --- Data Dirs ---\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, 'raw')\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, 'processed')\n",
    "\n",
    "# --- Model Dir ---\n",
    "MODELS_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "\n",
    "# --- Input File ---\n",
    "BINDINGDB_FILE = os.path.join(RAW_DATA_DIR, 'BindingDB_All.tsv')\n",
    "\n",
    "# --- Processed Data Files ---\n",
    "CLEANED_FILE = os.path.join(PROCESSED_DATA_DIR, 'cleaned_bindingdb_data.csv')\n",
    "FEATURES_X_FILE = os.path.join(PROCESSED_DATA_DIR, 'X_features.npy')\n",
    "FEATURES_Y_CLASS_FILE = os.path.join(PROCESSED_DATA_DIR, 'y_class.npy')\n",
    "FEATURES_Y_REG_FILE = os.path.join(PROCESSED_DATA_DIR, 'y_reg.npy')\n",
    "\n",
    "# --- Model Files ---\n",
    "MODEL_CLASSIFIER = os.path.join(MODELS_DIR, 'classifier.pkl')\n",
    "MODEL_REGRESSOR = os.path.join(MODELS_DIR, 'regressor.pkl')\n",
    "MODEL_SCALER = os.path.join(MODELS_DIR, 'scaler.pkl')\n",
    "MODEL_EXTRACTOR = os.path.join(MODELS_DIR, 'feature_extractor.pkl')\n",
    "\n",
    "# --- Create Dirs if they don't exist ---\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" 🚀 DRUG-TARGET INTERACTION ML TRAINING (XGBoost GPU - v5 IPYNB) 🚀\")\n",
    "print(f\" Mode: Native Windows (XGBoost) with Skip-Logic\")\n",
    "print(f\" Reading from: {RAW_DATA_DIR}\")\n",
    "print(f\" Saving to: {PROCESSED_DATA_DIR} and {MODELS_DIR}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff785170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== UTILITY FUNCTION ====================\n",
    "\n",
    "def find_column(columns, possible_names):\n",
    "    \"\"\"Find column by trying multiple possible names\"\"\"\n",
    "    for name in possible_names:\n",
    "        for col in columns:\n",
    "            if name.lower() in col.lower():\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "# ==================== STEP 1: LOAD BINDINGDB DATA ====================\n",
    "def load_bindingdb_data(filepath=BINDINGDB_FILE, sample_size=1500000):\n",
    "    \"\"\"Load and intelligently sample BindingDB TSV/CSV data\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 1: LOADING BINDINGDB DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"\\n❌ ERROR: File not found: {filepath}\")\n",
    "        print(f\"    Please ensure 'BindingDB_All.tsv' is in {RAW_DATA_DIR}\")\n",
    "        return None\n",
    "        \n",
    "    file_size_gb = os.path.getsize(filepath) / (1024**3)\n",
    "    print(f\"\\n📖 Reading file: {os.path.basename(filepath)}\")\n",
    "    print(f\"    File size: {file_size_gb:.2f} GB\")\n",
    "    print(f\"    Target sample size: {sample_size:,} entries\")\n",
    "    \n",
    "    print(\"\\n🧐 Analyzing file header to find required columns...\")\n",
    "    \n",
    "    sep = '\\t' if filepath.endswith('.tsv') else ','\n",
    "    delimiter_name = 'TAB (\\\\t)' if sep == '\\t' else 'COMMA (,)'\n",
    "    print(f\"    Detected delimiter: {delimiter_name}\")\n",
    "    \n",
    "    try:\n",
    "        header = pd.read_csv(filepath, sep=sep, nrows=0).columns\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ CRITICAL ERROR: Could not read file header: {e}\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"    File has {len(header)} total columns.\")\n",
    "\n",
    "    smiles_col = find_column(header, ['ligand smiles', 'smiles', 'ligand_smiles'])\n",
    "    sequence_col = find_column(header, ['target sequence', 'bindingdb target chain sequence', 'sequence', 'protein sequence'])\n",
    "    ic50_col = find_column(header, ['ic50 (nm)', 'ic50', 'ic50_nm'])\n",
    "    ki_col = find_column(header, ['ki (nm)', 'ki', 'ki_nm'])\n",
    "    kd_col = find_column(header, ['kd (nm)', 'kd', 'kd_nm'])\n",
    "    \n",
    "    cols_to_load = [smiles_col, sequence_col, ic50_col, ki_col, kd_col]\n",
    "    cols_to_load = [col for col in cols_to_load if col is not None]\n",
    "    \n",
    "    if not smiles_col or not sequence_col:\n",
    "        print(\"\\n❌ ERROR: Could not find required SMILES or Sequence column!\")\n",
    "        return None\n",
    "        \n",
    "    if not any([ic50_col, ki_col, kd_col]):\n",
    "        print(\"\\n❌ ERROR: Could not find any binding affinity column (IC50, Ki, Kd)!\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\n✓ Found required columns:\")\n",
    "    print(f\"    SMILES: {smiles_col}\")\n",
    "    print(f\"    Sequence: {sequence_col}\")\n",
    "    if ic50_col: print(f\"    IC50: {ic50_col}\")\n",
    "    if ki_col: print(f\"    Ki: {ki_col}\")\n",
    "    if kd_col: print(f\"    Kd: {kd_col}\")\n",
    "    \n",
    "    print(f\"\\n📦 Reading {file_size_gb:.2f}GB file in chunks...\")\n",
    "    print(f\"    This may take 5-10 minutes. Please wait...\")\n",
    "    \n",
    "    chunk_size = 1_000_000\n",
    "    chunks = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    try:\n",
    "        try:\n",
    "            print(\"    (Performing quick row count for progress bar...)\")\n",
    "            total_rows_estimate = sum(1 for row in open(filepath, 'r', encoding='utf-8'))\n",
    "            n_chunks = int(np.ceil(total_rows_estimate / chunk_size))\n",
    "            print(f\"    (Estimated {total_rows_estimate:,} rows in {n_chunks} chunks)\")\n",
    "        except:\n",
    "            n_chunks = None\n",
    "            print(\"    (Could not get row count, progress bar will be un-timed)\")\n",
    "\n",
    "        chunk_iter = pd.read_csv(\n",
    "            filepath, \n",
    "            sep=sep, \n",
    "            chunksize=chunk_size, \n",
    "            usecols=cols_to_load,\n",
    "            low_memory=False\n",
    "        )\n",
    "\n",
    "        for i, chunk in enumerate(tqdm(chunk_iter, total=n_chunks, desc=\"Reading 6GB File\")):\n",
    "            total_rows += len(chunk)\n",
    "            \n",
    "            if ic50_col:\n",
    "                binding_col = ic50_col\n",
    "            elif ki_col:\n",
    "                binding_col = ki_col\n",
    "            else:\n",
    "                binding_col = kd_col\n",
    "            \n",
    "            chunk = chunk.rename(columns={\n",
    "                smiles_col: 'drug_smiles',\n",
    "                sequence_col: 'protein_sequence',\n",
    "                binding_col: 'binding_value'\n",
    "            })\n",
    "            \n",
    "            chunk = chunk.dropna(subset=['drug_smiles', 'protein_sequence', 'binding_value'])\n",
    "            chunk = chunk[chunk['drug_smiles'].astype(str).str.len() > 5]\n",
    "            \n",
    "            seq_lens = chunk['protein_sequence'].astype(str).str.len()\n",
    "            chunk = chunk[(seq_lens >= 50) & (seq_lens <= 1500)]\n",
    "            \n",
    "            chunk['binding_value'] = pd.to_numeric(chunk['binding_value'], errors='coerce')\n",
    "            chunk = chunk.dropna(subset=['binding_value'])\n",
    "            \n",
    "            chunk = chunk[['drug_smiles', 'protein_sequence', 'binding_value']]\n",
    "            \n",
    "            if len(chunk) > 0:\n",
    "                chunks.append(chunk)\n",
    "\n",
    "        print(f\"\\n\\n    ✓ Finished reading. Total rows processed: {total_rows:,}\")\n",
    "        \n",
    "        if not chunks:\n",
    "            print(\"❌ ERROR: No valid data was found after filtering all chunks.\")\n",
    "            return None\n",
    "            \n",
    "        print(\"    Concatenating filtered chunks...\")\n",
    "        df = pd.concat(chunks, ignore_index=True)\n",
    "        del chunks\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"    ✓ Found {len(df):,} valid entries total.\")\n",
    "        \n",
    "        if len(df) > sample_size:\n",
    "            print(f\"\\n📉 Sampling {sample_size:,} from {len(df):,} valid entries...\")\n",
    "            df = df.sample(sample_size, random_state=42)\n",
    "        else:\n",
    "            print(f\"\\n✓ Using all {len(df):,} valid entries (less than target)\")\n",
    "            \n",
    "        print(f\"\\n✅ DATA LOADED SUCCESSFULLY:\")\n",
    "        print(f\"    Total entries: {len(df):,}\")\n",
    "        print(f\"    Memory usage: ~{df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERROR loading file: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# ==================== STEP 2: CLEAN DATA ====================\n",
    "def clean_bindingdb_data(df):\n",
    "    \"\"\"Clean and process the already-filtered BindingDB data\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 2: CLEANING AND PROCESSING DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n🧹 Processing {len(df):,} pre-filtered entries\")\n",
    "    \n",
    "    print(f\"\\n📊 Processing binding affinities...\")\n",
    "    \n",
    "    df['binding_affinity'] = df['binding_value'] / 1000.0\n",
    "    \n",
    "    initial = len(df)\n",
    "    df = df[(df['binding_affinity'] > 0.001) & (df['binding_affinity'] < 100000)]\n",
    "    print(f\"    ✓ Valid binding values: {len(df):,} entries ({len(df)/initial*100:.1f}% kept)\")\n",
    "    \n",
    "    df['binds'] = (df['binding_affinity'] < 10).astype(int)\n",
    "    \n",
    "    print(f\"\\n🧬 Validating protein sequences...\")\n",
    "    valid_aa = set('ACDEFGHIKLMNPQRSTVWYXU-')\n",
    "    \n",
    "    def is_valid_sequence(seq):\n",
    "        if not isinstance(seq, str) or len(seq) == 0:\n",
    "            return False\n",
    "        seq = seq.strip().upper()\n",
    "        valid_count = sum(1 for aa in seq if aa in valid_aa)\n",
    "        return valid_count / len(seq) >= 0.95\n",
    "    \n",
    "    print(\"    (This may take a minute...)\")\n",
    "    valid_mask = df['protein_sequence'].progress_apply(is_valid_sequence)\n",
    "    df = df[valid_mask]\n",
    "    print(f\"    ✓ Valid sequences: {len(df):,} entries\")\n",
    "    \n",
    "    df['protein_sequence'] = df['protein_sequence'].str.strip().str.upper()\n",
    "    df['drug_smiles'] = df['drug_smiles'].str.strip()\n",
    "    \n",
    "    initial = len(df)\n",
    "    df = df.drop_duplicates(subset=['protein_sequence', 'drug_smiles'])\n",
    "    if len(df) < initial:\n",
    "        print(f\"    ✓ Removed {initial - len(df):,} duplicates\")\n",
    "    \n",
    "    df = df[['protein_sequence', 'drug_smiles', 'binding_affinity', 'binds']].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n✅ FINAL CLEANED DATASET:\")\n",
    "    print(f\"    Total samples: {len(df):,}\")\n",
    "    print(f\"    Binders (IC50 < 10 μM): {df['binds'].sum():,} ({df['binds'].sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"    Non-binders: {(1-df['binds']).sum():,} ({(1-df['binds']).sum()/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # ### --- PATH CHANGED --- ###\n",
    "    df.to_csv(CLEANED_FILE, index=False)\n",
    "    print(f\"\\n💾 Saved cleaned data to: {CLEANED_FILE}\")\n",
    "    # ### --- END CHANGED --- ###\n",
    "    \n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2674289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STEP 3: FEATURE EXTRACTION ====================\n",
    "\n",
    "class FeatureExtractor:\n",
    "    \"\"\"Extract numerical features from proteins and drugs\"\"\"\n",
    "    # This class is self-contained in the notebook\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.aa_weights = {\n",
    "            'A': 89, 'C': 121, 'D': 133, 'E': 147, 'F': 165,\n",
    "            'G': 75, 'H': 155, 'I': 131, 'K': 146, 'L': 131,\n",
    "            'M': 149, 'N': 132, 'P': 115, 'Q': 146, 'R': 174,\n",
    "            'S': 105, 'T': 119, 'V': 117, 'W': 204, 'Y': 181\n",
    "        }\n",
    "        self.amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "\n",
    "    def protein_to_features(self, sequence):\n",
    "        \"\"\"Convert protein sequence to 33 numerical features\"\"\"\n",
    "        sequence = ''.join([aa for aa in sequence if aa in self.amino_acids])\n",
    "        \n",
    "        if len(sequence) == 0:\n",
    "            return None\n",
    "            \n",
    "        total = len(sequence)\n",
    "        features = []\n",
    "        \n",
    "        aa_counts = Counter(sequence)\n",
    "        for aa in self.amino_acids:\n",
    "            features.append(aa_counts[aa] / total)\n",
    "            \n",
    "        features.append(total) # Length\n",
    "        features.append(sum([self.aa_weights.get(aa, 110) for aa in sequence])) # MW\n",
    "        features.append((aa_counts['F'] + aa_counts['W'] + aa_counts['Y']) / total) # Aromaticity\n",
    "        \n",
    "        if len(sequence) > 1:\n",
    "            dipeptides = [sequence[i:i+2] for i in range(len(sequence)-1)]\n",
    "            dipeptide_counts = Counter(dipeptides)\n",
    "            most_common = dipeptide_counts.most_common(10)\n",
    "            for i in range(10):\n",
    "                if i < len(most_common):\n",
    "                    features.append(most_common[i][1] / (total - 1))\n",
    "                else:\n",
    "                    features.append(0)\n",
    "        else:\n",
    "            features.extend([0] * 10)\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def smiles_to_features(self, smiles):\n",
    "        \"\"\"Convert SMILES to 17 numerical features\"\"\"\n",
    "        features = []\n",
    "        features.append(len(smiles)) # Length\n",
    "        features.append(smiles.count('C')) # Carbons\n",
    "        features.append(smiles.count('O')) # Oxygens\n",
    "        features.append(smiles.count('N')) # Nitrogens\n",
    "        features.append(smiles.count('S')) # Sulfurs\n",
    "        features.append(smiles.count('P')) # Phosphorus\n",
    "        features.append(smiles.count('=')) # Double bonds\n",
    "        features.append(smiles.count('#')) # Triple bonds\n",
    "        features.append(smiles.count('(')) # Branches\n",
    "        features.append(smiles.count('[')) # Atoms in brackets\n",
    "        features.append(smiles.count('@')) # Chirality\n",
    "        \n",
    "        for i in range(1, 7):\n",
    "            features.append(smiles.count(str(i)))\n",
    "            \n",
    "        return features\n",
    "    \n",
    "    def combine_features(self, protein_features, drug_features):\n",
    "        \"\"\"Combine protein and drug features\"\"\"\n",
    "        return protein_features + drug_features\n",
    "\n",
    "def prepare_ml_features(df):\n",
    "    \"\"\"Convert dataframe to ML-ready feature matrix\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 3: EXTRACTING ML FEATURES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    extractor = FeatureExtractor()\n",
    "    \n",
    "    total = len(df)\n",
    "    print(f\"\\n🔬 Processing {total:,} samples...\")\n",
    "    \n",
    "    print(\"    Extracting protein features (with progress bar)...\")\n",
    "    prot_features = df['protein_sequence'].progress_apply(extractor.protein_to_features)\n",
    "    \n",
    "    print(\"    Extracting drug features (with progress bar)...\")\n",
    "    drug_features = df['drug_smiles'].progress_apply(extractor.smiles_to_features)\n",
    "    \n",
    "    print(\"    Combining features...\")\n",
    "    df_features = pd.DataFrame({\n",
    "        'prot': prot_features,\n",
    "        'drug': drug_features,\n",
    "        'y_class': df['binds'],\n",
    "        'y_reg': df['binding_affinity']\n",
    "    })\n",
    "    \n",
    "    df_features = df_features.dropna().reset_index(drop=True)\n",
    "    \n",
    "    failed = total - len(df_features)\n",
    "    \n",
    "    X_list = df_features.progress_apply(lambda row: extractor.combine_features(row['prot'], row['drug']), axis=1)\n",
    "    X = np.array(X_list.tolist())\n",
    "    y_class = df_features['y_class'].values\n",
    "    y_reg = df_features['y_reg'].values\n",
    "    \n",
    "    print(f\"\\n    ✓ Successfully processed: {len(X):,} samples\")\n",
    "    if failed > 0:\n",
    "        print(f\"    ⚠ Failed to process: {failed} samples ({failed/total*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n✅ FEATURE EXTRACTION COMPLETE:\")\n",
    "    print(f\"    Feature matrix shape: {X.shape}\")\n",
    "    print(f\"    Memory: ~{X.nbytes / 1024**2:.1f} MB\")\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    # ### --- PATH CHANGED --- ###\n",
    "    # Save the outputs of this step for faster re-runs\n",
    "    print(\"\\n💾 Saving feature-extracted data for faster re-runs...\")\n",
    "    try:\n",
    "        np.save(FEATURES_X_FILE, X)\n",
    "        print(f\"    ✓ Saved {os.path.basename(FEATURES_X_FILE)}\")\n",
    "        np.save(FEATURES_Y_CLASS_FILE, y_class)\n",
    "        print(f\"    ✓ Saved {os.path.basename(FEATURES_Y_CLASS_FILE)}\")\n",
    "        np.save(FEATURES_Y_REG_FILE, y_reg)\n",
    "        print(f\"    ✓ Saved {os.path.basename(FEATURES_Y_REG_FILE)}\")\n",
    "        \n",
    "        with open(MODEL_EXTRACTOR, 'wb') as f:\n",
    "            pickle.dump(extractor, f)\n",
    "        print(f\"    ✓ Saved {os.path.basename(MODEL_EXTRACTOR)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ⚠ Warning: Could not save feature files: {e}\")\n",
    "    # ### --- END CHANGED --- ###\n",
    "    \n",
    "    return X, y_class, y_reg, extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c9bd15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== STEP 4: TRAIN ML MODELS (XGBOOST GPU) ====================\n",
    "\n",
    "def train_models(X, y_class, y_reg, extractor):\n",
    "    \"\"\"Train XGBoost models on the GPU\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 4: TRAINING MACHINE LEARNING MODELS\")\n",
    "    print(\"        🚀 STATUS: XGBoost (Native Windows GPU) 🚀\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n📊 Scaling features...\")\n",
    "    scaler = StandardScaler() \n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    print(\"    ✓ Features scaled (mean=0, std=1)\")\n",
    "    \n",
    "    # ===== TRAIN CLASSIFIER =====\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TRAINING XGBoost CLASSIFIER (Binds: Yes/No)\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n    Training set: {len(X_train):,} samples\")\n",
    "    print(f\"    Test set: {len(X_test):,} samples\")\n",
    "    \n",
    "    classifier = xgb.XGBClassifier(\n",
    "        tree_method='hist', device='cuda',\n",
    "        n_estimators=1000,\n",
    "        max_depth=12,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "    \n",
    "    print(\"\\n    🌲 Training XGBoost Classifier...\")\n",
    "    print(\"      (This will take a few minutes)\")\n",
    "    \n",
    "    classifier.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        verbose=50\n",
    "    )\n",
    "    print(\"    ✓ Training complete!\")\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, classifier.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, classifier.predict(X_test))\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print(f\"\\n    📈 CLASSIFIER RESULTS:\")\n",
    "    print(f\"       Training Accuracy: {train_acc:.3f} ({train_acc*100:.1f}%)\")\n",
    "    print(f\"       Testing Accuracy: {test_acc:.3f} ({test_acc*100:.1f}%)\")\n",
    "    print(f\"\\n    Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred,\n",
    "                                target_names=['No Binding', 'Binds'],\n",
    "                                digits=3))\n",
    "    \n",
    "    # ===== TRAIN REGRESSOR =====\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TRAINING XGBoost REGRESSOR (Binding Affinity Prediction)\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_reg, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n    Training set: {len(X_train):,} samples\")\n",
    "    print(f\"    Test set: {len(X_test):,} samples\")\n",
    "    \n",
    "    regressor = xgb.XGBRegressor(\n",
    "        tree_method='hist', device='cuda',\n",
    "        n_estimators=1000,\n",
    "        max_depth=12,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "    \n",
    "    print(\"\\n    🌲 Training XGBoost Regressor...\")\n",
    "    print(\"      (This will take a few minutes)\")\n",
    "    \n",
    "    regressor.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        verbose=50\n",
    "    )\n",
    "    print(\"    ✓ Training complete!\")\n",
    "    \n",
    "    train_r2 = r2_score(y_train, regressor.predict(X_train))\n",
    "    test_r2 = r2_score(y_test, regressor.predict(X_test))\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = np.mean(np.abs(y_test - y_pred))\n",
    "    \n",
    "    print(f\"\\n    📈 REGRESSOR RESULTS:\")\n",
    "    print(f\"       Training R²: {train_r2:.3f}\")\n",
    "    print(f\"       Testing R²: {test_r2:.3f}\")\n",
    "    print(f\"       RMSE: {rmse:.3f} μM\")\n",
    "    print(f\"       MAE: {mae:.3f} μM\")\n",
    "    \n",
    "    # ### --- PATH CHANGED --- ###\n",
    "    # Save models to the /models directory\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"💾 SAVING MODELS to {MODELS_DIR}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    with open(MODEL_CLASSIFIER, 'wb') as f:\n",
    "        pickle.dump(classifier, f)\n",
    "    print(f\"    ✓ {os.path.basename(MODEL_CLASSIFIER)}\")\n",
    "    \n",
    "    with open(MODEL_REGRESSOR, 'wb') as f:\n",
    "        pickle.dump(regressor, f)\n",
    "    print(f\"    ✓ {os.path.basename(MODEL_REGRESSOR)}\")\n",
    "    \n",
    "    with open(MODEL_SCALER, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f\"    ✓ {os.path.basename(MODEL_SCALER)}\")\n",
    "    # ### --- END CHANGED --- ###\n",
    "        \n",
    "    return classifier, regressor, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3443d576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "♻️ SKIPPING TO STEP 3: Found cleaned_bindingdb_data.csv!\n",
      "    Loading ../data\\processed\\cleaned_bindingdb_data.csv...\n",
      "    ✓ Loaded 1,122,930 cleaned samples.\n",
      "\n",
      "================================================================================\n",
      "STEP 3: EXTRACTING ML FEATURES\n",
      "================================================================================\n",
      "\n",
      "🔬 Processing 1,122,930 samples...\n",
      "    Extracting protein features (with progress bar)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1122930/1122930 [05:01<00:00, 3727.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracting drug features (with progress bar)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1122930/1122930 [00:07<00:00, 141517.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Combining features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1122930/1122930 [00:11<00:00, 96975.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ✓ Successfully processed: 1,122,930 samples\n",
      "\n",
      "✅ FEATURE EXTRACTION COMPLETE:\n",
      "    Feature matrix shape: (1122930, 50)\n",
      "    Memory: ~428.4 MB\n",
      "\n",
      "💾 Saving feature-extracted data for faster re-runs...\n",
      "    ✓ Saved X_features.npy\n",
      "    ✓ Saved y_class.npy\n",
      "    ✓ Saved y_reg.npy\n",
      "    ✓ Saved feature_extractor.pkl\n",
      "\n",
      "Starting Model Training...\n",
      "\n",
      "================================================================================\n",
      "STEP 4: TRAINING MACHINE LEARNING MODELS\n",
      "        🚀 STATUS: XGBoost (Native Windows GPU) 🚀\n",
      "================================================================================\n",
      "\n",
      "📊 Scaling features...\n",
      "    ✓ Features scaled (mean=0, std=1)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRAINING XGBoost CLASSIFIER (Binds: Yes/No)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    Training set: 898,344 samples\n",
      "    Test set: 224,586 samples\n",
      "\n",
      "    🌲 Training XGBoost Classifier...\n",
      "      (This will take a few minutes)\n",
      "[0]\tvalidation_0-logloss:0.41185\n",
      "[50]\tvalidation_0-logloss:0.27284\n",
      "[100]\tvalidation_0-logloss:0.24958\n",
      "[150]\tvalidation_0-logloss:0.24142\n",
      "[200]\tvalidation_0-logloss:0.23610\n",
      "[250]\tvalidation_0-logloss:0.23224\n",
      "[300]\tvalidation_0-logloss:0.22827\n",
      "[350]\tvalidation_0-logloss:0.22556\n",
      "[400]\tvalidation_0-logloss:0.22315\n",
      "[450]\tvalidation_0-logloss:0.22153\n",
      "[500]\tvalidation_0-logloss:0.22010\n",
      "[550]\tvalidation_0-logloss:0.21878\n",
      "[600]\tvalidation_0-logloss:0.21771\n",
      "[650]\tvalidation_0-logloss:0.21689\n",
      "[700]\tvalidation_0-logloss:0.21609\n",
      "[750]\tvalidation_0-logloss:0.21560\n",
      "[800]\tvalidation_0-logloss:0.21504\n",
      "[850]\tvalidation_0-logloss:0.21473\n",
      "[900]\tvalidation_0-logloss:0.21444\n",
      "[950]\tvalidation_0-logloss:0.21433\n",
      "[999]\tvalidation_0-logloss:0.21423\n",
      "    ✓ Training complete!\n",
      "\n",
      "    📈 CLASSIFIER RESULTS:\n",
      "       Training Accuracy: 0.957 (95.7%)\n",
      "       Testing Accuracy: 0.912 (91.2%)\n",
      "\n",
      "    Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Binding      0.766     0.558     0.646     32354\n",
      "       Binds      0.929     0.971     0.950    192232\n",
      "\n",
      "    accuracy                          0.912    224586\n",
      "   macro avg      0.847     0.765     0.798    224586\n",
      "weighted avg      0.905     0.912     0.906    224586\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRAINING XGBoost REGRESSOR (Binding Affinity Prediction)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    Training set: 898,344 samples\n",
      "    Test set: 224,586 samples\n",
      "\n",
      "    🌲 Training XGBoost Regressor...\n",
      "      (This will take a few minutes)\n",
      "[0]\tvalidation_0-rmse:1109.31161\n",
      "[50]\tvalidation_0-rmse:994.15122\n",
      "[100]\tvalidation_0-rmse:985.58686\n",
      "[145]\tvalidation_0-rmse:988.02060\n",
      "    ✓ Training complete!\n",
      "\n",
      "    📈 REGRESSOR RESULTS:\n",
      "       Training R²: 0.642\n",
      "       Testing R²: 0.224\n",
      "       RMSE: 984.771 μM\n",
      "       MAE: 62.287 μM\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "💾 SAVING MODELS to ../models\n",
      "--------------------------------------------------------------------------------\n",
      "    ✓ classifier.pkl\n",
      "    ✓ regressor.pkl\n",
      "    ✓ scaler.pkl\n",
      "\n",
      "================================================================================\n",
      " ✅ TRAINING COMPLETE! (XGBoost GPU - v5 IPYNB)\n",
      "================================================================================\n",
      "\n",
      "📦 Created files in ../models:\n",
      "    ✓ classifier.pkl\n",
      "    ✓ regressor.pkl\n",
      "    ✓ scaler.pkl\n",
      "    ✓ feature_extractor.pkl\n",
      "\n",
      "📦 Created files in ../data\\processed:\n",
      "    ✓ cleaned_bindingdb_data.csv\n",
      "    ✓ X_features.npy, y_class.npy, y_reg.npy\n",
      "\n",
      "🚀 Next steps:\n",
      "    1. Your models are ready to use!\n",
      "    2. Your streamlit app (src/app.py) can now load the models from ../models.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "♻️ SKIPPING TO STEP 4: Found pre-computed feature files!\n",
      "    Loading from ../data\\processed and ../models...\n",
      "    ✓ Loaded features with shape: (1122930, 50)\n",
      "\n",
      "Starting Model Training...\n",
      "\n",
      "================================================================================\n",
      "STEP 4: TRAINING MACHINE LEARNING MODELS\n",
      "        🚀 STATUS: XGBoost (Native Windows GPU) 🚀\n",
      "================================================================================\n",
      "\n",
      "📊 Scaling features...\n",
      "    ✓ Features scaled (mean=0, std=1)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRAINING XGBoost CLASSIFIER (Binds: Yes/No)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    Training set: 898,344 samples\n",
      "    Test set: 224,586 samples\n",
      "\n",
      "    🌲 Training XGBoost Classifier...\n",
      "      (This will take a few minutes)\n",
      "[0]\tvalidation_0-logloss:0.41185\n",
      "[50]\tvalidation_0-logloss:0.27284\n",
      "[100]\tvalidation_0-logloss:0.24958\n",
      "[150]\tvalidation_0-logloss:0.24142\n",
      "[200]\tvalidation_0-logloss:0.23610\n",
      "[250]\tvalidation_0-logloss:0.23224\n",
      "[300]\tvalidation_0-logloss:0.22827\n",
      "[350]\tvalidation_0-logloss:0.22556\n",
      "[400]\tvalidation_0-logloss:0.22315\n",
      "[450]\tvalidation_0-logloss:0.22153\n",
      "[500]\tvalidation_0-logloss:0.22010\n",
      "[550]\tvalidation_0-logloss:0.21878\n",
      "[600]\tvalidation_0-logloss:0.21771\n",
      "[650]\tvalidation_0-logloss:0.21689\n",
      "[700]\tvalidation_0-logloss:0.21609\n",
      "[750]\tvalidation_0-logloss:0.21560\n",
      "[800]\tvalidation_0-logloss:0.21504\n",
      "[850]\tvalidation_0-logloss:0.21473\n",
      "[900]\tvalidation_0-logloss:0.21444\n",
      "[950]\tvalidation_0-logloss:0.21433\n",
      "[999]\tvalidation_0-logloss:0.21423\n",
      "    ✓ Training complete!\n",
      "\n",
      "    📈 CLASSIFIER RESULTS:\n",
      "       Training Accuracy: 0.957 (95.7%)\n",
      "       Testing Accuracy: 0.912 (91.2%)\n",
      "\n",
      "    Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Binding      0.766     0.558     0.646     32354\n",
      "       Binds      0.929     0.971     0.950    192232\n",
      "\n",
      "    accuracy                          0.912    224586\n",
      "   macro avg      0.847     0.765     0.798    224586\n",
      "weighted avg      0.905     0.912     0.906    224586\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TRAINING XGBoost REGRESSOR (Binding Affinity Prediction)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "    Training set: 898,344 samples\n",
      "    Test set: 224,586 samples\n",
      "\n",
      "    🌲 Training XGBoost Regressor...\n",
      "      (This will take a few minutes)\n",
      "[0]\tvalidation_0-rmse:1109.31161\n",
      "[50]\tvalidation_0-rmse:994.15122\n",
      "[100]\tvalidation_0-rmse:985.58686\n",
      "[145]\tvalidation_0-rmse:988.02060\n",
      "    ✓ Training complete!\n",
      "\n",
      "    📈 REGRESSOR RESULTS:\n",
      "       Training R²: 0.642\n",
      "       Testing R²: 0.224\n",
      "       RMSE: 984.771 μM\n",
      "       MAE: 62.287 μM\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "💾 SAVING MODELS to ../models\n",
      "--------------------------------------------------------------------------------\n",
      "    ✓ classifier.pkl\n",
      "    ✓ regressor.pkl\n",
      "    ✓ scaler.pkl\n",
      "\n",
      "================================================================================\n",
      " ✅ TRAINING COMPLETE! (XGBoost GPU - v5 IPYNB)\n",
      "================================================================================\n",
      "\n",
      "📦 Created files in ../models:\n",
      "    ✓ classifier.pkl\n",
      "    ✓ regressor.pkl\n",
      "    ✓ scaler.pkl\n",
      "    ✓ feature_extractor.pkl\n",
      "\n",
      "📦 Created files in ../data\\processed:\n",
      "    ✓ cleaned_bindingdb_data.csv\n",
      "    ✓ X_features.npy, y_class.npy, y_reg.npy\n",
      "\n",
      "🚀 Next steps:\n",
      "    1. Your models are ready to use!\n",
      "    2. Your streamlit app (src/app.py) can now load the models from ../models.\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ==================== MAIN EXECUTION ====================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    \n",
    "    # All path variables are defined in Cell 1\n",
    "    \n",
    "    SAMPLE_SIZE = 1_500_000 \n",
    "    \n",
    "    X, y_class, y_reg, extractor = None, None, None, None\n",
    "    df = None\n",
    "\n",
    "    # === CHECK 1: SKIP TO STEP 4 (Training) ===\n",
    "    feature_files = [FEATURES_X_FILE, FEATURES_Y_CLASS_FILE, FEATURES_Y_REG_FILE, MODEL_EXTRACTOR]\n",
    "    \n",
    "    if all(os.path.exists(f) for f in feature_files):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"♻️ SKIPPING TO STEP 4: Found pre-computed feature files!\")\n",
    "        print(f\"    Loading from {PROCESSED_DATA_DIR} and {MODELS_DIR}...\")\n",
    "        try:\n",
    "            X = np.load(FEATURES_X_FILE)\n",
    "            y_class = np.load(FEATURES_Y_CLASS_FILE)\n",
    "            y_reg = np.load(FEATURES_Y_REG_FILE)\n",
    "            with open(MODEL_EXTRACTOR, 'rb') as f:\n",
    "                extractor = pickle.load(f)\n",
    "            print(f\"    ✓ Loaded features with shape: {X.shape}\")\n",
    "            \n",
    "            if extractor is None:\n",
    "                print(f\"    ...Missing {os.path.basename(MODEL_EXTRACTOR)}. Re-computing...\")\n",
    "                X = None # Force re-computation\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ❌ Error loading feature files: {e}. Re-computing...\")\n",
    "            X = None # Force re-computation\n",
    "\n",
    "    # === CHECK 2: SKIP TO STEP 3 (Feature Extraction) ===\n",
    "    if X is None and os.path.exists(CLEANED_FILE):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"♻️ SKIPPING TO STEP 3: Found {os.path.basename(CLEANED_FILE)}!\")\n",
    "        print(f\"    Loading {CLEANED_FILE}...\")\n",
    "        try:\n",
    "            df = pd.read_csv(CLEANED_FILE)\n",
    "            if len(df) < 1000:\n",
    "                print(\"    ...File is too small. Re-running from start.\")\n",
    "                df = None # Force re-run\n",
    "            else:\n",
    "                print(f\"    ✓ Loaded {len(df):,} cleaned samples.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ❌ Error loading {os.path.basename(CLEANED_FILE)}: {e}. Re-running from start...\")\n",
    "            df = None # Force re-run\n",
    "\n",
    "    # === CHECK 3: Run from STEP 1 (Full Load) ===\n",
    "    if df is None and X is None:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"▶️ STARTING FROM STEP 1: No valid cached data found.\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        df = load_bindingdb_data(filepath=BINDINGDB_FILE, sample_size=SAMPLE_SIZE)\n",
    "        \n",
    "        if df is None or len(df) == 0:\n",
    "            print(\"\\n❌ Failed to load valid data. Exiting...\")\n",
    "            return\n",
    "            \n",
    "        df = clean_bindingdb_data(df)\n",
    "        if df is None or len(df) < 1000:\n",
    "            print(f\"\\n❌ Not enough valid data ({len(df) if df is not None else 0} samples).\")\n",
    "            return\n",
    "    \n",
    "    # --- RUN STEP 3 (if needed) ---\n",
    "    if X is None:\n",
    "        X, y_class, y_reg, extractor = prepare_ml_features(df)\n",
    "    \n",
    "    # --- RUN STEP 4 (Training) ---\n",
    "    if X is None:\n",
    "        print(\"\\n❌ Critical error: Feature matrix (X) is still None. Exiting.\")\n",
    "        return\n",
    "        \n",
    "    print(\"\\nStarting Model Training...\")\n",
    "    classifier, regressor, scaler = train_models(X, y_class, y_reg, extractor)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" ✅ TRAINING COMPLETE! (XGBoost GPU - v5 IPYNB)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\n📦 Created files in {MODELS_DIR}:\")\n",
    "    print(f\"    ✓ {os.path.basename(MODEL_CLASSIFIER)}\")\n",
    "    print(f\"    ✓ {os.path.basename(MODEL_REGRESSOR)}\")\n",
    "    print(f\"    ✓ {os.path.basename(MODEL_SCALER)}\")\n",
    "    print(f\"    ✓ {os.path.basename(MODEL_EXTRACTOR)}\")\n",
    "    print(f\"\\n📦 Created files in {PROCESSED_DATA_DIR}:\")\n",
    "    print(f\"    ✓ {os.path.basename(CLEANED_FILE)}\")\n",
    "    print(f\"    ✓ {os.path.basename(FEATURES_X_FILE)}, {os.path.basename(FEATURES_Y_CLASS_FILE)}, {os.path.basename(FEATURES_Y_REG_FILE)}\")\n",
    "    \n",
    "    print(\"\\n🚀 Next steps:\")\n",
    "    print(f\"    1. Your models are ready to use!\")\n",
    "    print(f\"    2. Your streamlit app (src/app.py) can now load the models from {MODELS_DIR}.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# --- Run the pipeline ---\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n⚠ Training interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\n❌ UNEXPECTED ERROR:\")\n",
    "        print(f\"    {e}\")\n",
    "        print(\"\\nFull traceback:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# This final `main()` call is what triggers the whole pipeline\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dti_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
